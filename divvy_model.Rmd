---
title: "divvy_model"
author: "Sarah Deussing, Imogen Meers, Sarah Cernugel"
date: "2024-11-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(data.table)
divvy <- as.data.frame(fread("divvy_data2223.csv"))
```

```{r}
head(divvy)
colnames(divvy)
```

```{r}
library(dplyr)

```



```{r Cleaning Data}
bad_stations <- c('WEST CHI-WATSON',
'Pawel Bialowas - Test- PBSC charging station',
'Divvy Valet - Oakwood Beach',
'DIVVY CASSETTE REPAIR MOBILE STATION',
'Base - 2132 W Hubbard',
'Base - 2132 W Hubbard Warehouse',
'NewHastings',
'WestChi',
'Hastings WH 2', 'MTV WH - Cassette Repair')
dim(divvy)

divvy <- divvy[!divvy$start_station_name %in% bad_stations, ]
divvy <- divvy[!divvy$end_station_name %in% bad_stations, ]


# List of substrings to remove
substr_to_remove <- c("Public Rack -", "Public  Rack -", "Pubic Rack -", "City Rack -", "\\*", "- Charging", "- midblock", "- midblock south", "\\(Temp\\)", "amp;") 
# Function to remove substrings
remove_substrings <- function(x, substrings) {
  for (substr in substrings) { x <- gsub(substr, "", x) } 
  return(x) } # Apply the function to the 'name' column 

divvy <- divvy %>% mutate(start_station_name = remove_substrings(start_station_name, substr_to_remove), end_station_name = remove_substrings(end_station_name, substr_to_remove) )

#make sure all statiosn with same name have same id
start_stat <- divvy[,c(4,5)] %>% distinct() %>% arrange(desc(start_station_name)) %>% 
  group_by(start_station_name) %>% slice_head(n = 1) %>% ungroup()


end_stat <- divvy[,c(6,7)] %>% distinct() %>% arrange(desc(end_station_name)) %>% 
  group_by(end_station_name) %>% slice_head(n = 1) %>% ungroup()

divvy <- divvy %>% rename(old_start = start_station_id, old_end = end_station_id)

divvy <- inner_join(divvy, start_stat, by = 'start_station_name')
divvy <- inner_join(divvy, end_stat, by = 'end_station_name')

divvy <- divvy %>% select(-old_start, -old_end)

#make sure all remaining ids have the same id and same name
start_stat <- start_stat %>% arrange(desc(start_station_name)) %>% 
  group_by(start_station_id) %>% slice_head(n = 1) %>% ungroup()

end_stat <- end_stat %>% arrange(desc(end_station_name)) %>% 
  group_by(end_station_id) %>% slice_head(n = 1) %>% ungroup()


divvy <- divvy %>% rename(old_start = start_station_name, old_end = end_station_name)

divvy <- inner_join(divvy, start_stat, by = 'start_station_id')
divvy <- inner_join(divvy, end_stat, by = 'end_station_id')

divvy <- divvy %>% select(-old_start, -old_end)
divvy


stats <- full_join(start_stat, end_stat, by = c("start_station_name" = "end_station_name"))
summary(stats[stats$start_station_id != stats$end_station_id,])

```

## Neighborhoods
```{r}
neighborhoods <- read.csv("Neighborhoods_2012b.csv")

write.csv(divvy, "clean_divvy.csv")

clean_divvy <- read.csv("clean_divvy.csv")

clean_divvy <- clean_divvy %>% filter(rideable_type == "classic_bike")


# Loop through each unique start_station_name
for(station in unique(clean_divvy$start_station_name)) {
  
  # Subset the data for the current station
  station_data <- clean_divvy[clean_divvy$start_station_name == station, ]
  
  # Calculate the average latitude and longitude
  avg_lat <- mean(station_data$start_lat)
  avg_long <- mean(station_data$start_lng)
  
  # Add the averaged values back to the original dataset
  clean_divvy[clean_divvy$start_station_name == station, "avg_start_lat"] <- avg_lat
  clean_divvy[clean_divvy$start_station_name == station, "avg_start_long"] <- avg_long
}


# Loop through each unique start_station_name
for(station in unique(clean_divvy$end_station_name)) {
  
  # Subset the data for the current station
  station_data <- clean_divvy[clean_divvy$end_station_name == station, ]
  
  # Calculate the average latitude and longitude
  avg_lat <- mean(station_data$start_lat)
  avg_long <- mean(station_data$start_lng)
  
  # Add the averaged values back to the original dataset
  clean_divvy[clean_divvy$end_station_name == station, "avg_end_lat"] <- avg_lat
  clean_divvy[clean_divvy$end_station_name == station, "avg_end_long"] <- avg_long
}

# View the updated dataset
print(clean_divvy)

write.csv(clean_divvy, "clean_divvy_points.csv")


for i in range(len(divvy)):  # Equivalent to iterating over the number of rows
    # Access the lat and long values for the current row using .iloc
    lat = divvy.iloc[i]['avg_end_lat']
    long = divvy.iloc[i]['avg_end_long']
    
    # Find the neighborhood for the current lat, long
    neighborhood = find_neighborhood(lat, long, all_neighborhoods)
    
    # Append the neighborhood to the list
    divvy_neighborhood.append(neighborhood)
    
    # Optionally print the neighborhood for each row
    print(f"Row {i}: Nearest Neighborhood: {neighborhood}")


for i in neighborhoods$the_geom {
  
}


# Load the necessary library
library(sf)

# Define the MULTIPOLYGON WKT (Well-Known Text) representation
wkt <- "MULTIPOLYGON (((-87.62760697485348 41.8743709778537, -87.62759525663328 41.87386171244117, 
                        -87.62756611032268 41.87309193343395, -87.6275551301491 41.87280194101277, 
                        -87.62754038267394 41.8723026159864, -87.6275257358244 41.87180670894443, 
                        -87.62751740010026 41.87152447340548, -87.62749380061312 41.870533289913496, 
                        -87.62748640976554 41.87022285721285, -87.62747968351995 41.8698699731487, 
                        -87.62746758964475 41.86923545315862, -87.62746178584436 41.86893095552231, 
                        -87.62741867651467 41.86741358880028, -87.62776525665168 41.86740884836574, 
                        -87.62791651412788 41.86740677917382, -87.62835789379616 41.867400574187975, 
                        -87.62847483044938 41.86739872168006, -87.6285016555064 41.86739829657602, 
                        -87.62886489441746 41.867392540895025, -87.6294510227246 41.86738627247538, 
                        -87.62994146063357 41.86738130883445, -87.63043520045348 41.867373985964946, 
                        -87.63045643033234 41.86832408508193, -87.63046103823288 41.86849676575636, 
                        -87.63047842774165 41.86914840420451, -87.63049175239173 41.86964771496229, 
                        -87.63049478033787 41.86968430855481, -87.63051202720345 41.8698927483438, 
                        -87.63055740862784 41.870441204350946, -87.63056128061226 41.87050901480099, 
                        -87.63056510977249 41.870576076357324, -87.63057023511439 41.87077857857081, 
                        -87.63057420023388 41.8709260784126, -87.63057428682792 41.870929267757866, 
                        -87.63058149972416 41.87119486693879, -87.63058916097859 41.87146415217918, 
                        -87.63060278276006 41.8719334194333, -87.63061066642632 41.87223168661656, 
                        -87.6306169562931 41.87246964964144, -87.630621108501 41.87264366027951, 
                        -87.63062301810568 41.87272407838625, -87.63062680325942 41.8728578561184, 
                        -87.63063080101777 41.87297892928771, -87.63063388056597 41.87307321326662, 
                        -87.63063923702379 41.87323449767777, -87.63064223866382 41.87336945062947, 
                        -87.63064686470452 41.87359738857438, -87.63065906168319 41.87401239319246, 
                        -87.63066229700475 41.87411274261591, -87.63067547794128 41.87452163354843, 
                        -87.63028379674365 41.87452623421518, -87.62966890774102 41.874534235064694, 
                        -87.6294635167731 41.87453690670954, -87.62923044428344 41.874539213980704, 
                        -87.62920969690175 41.874539419377946, -87.62891739196219 41.874542312338946, 
                        -87.62870043471254 41.874544998116875, -87.6286855987505 41.87454518151181, 
                        -87.62827705848584 41.87455023802575, -87.62796784305239 41.87455500774031, 
                        -87.62766945095747 41.87455282499508, -87.62766825002093 41.8745528159692, 
                        -87.62761115198867 41.87455239842161, -87.62760697485348 41.8743709778537)))"

# Convert the WKT string into an sf object
multipolygon <- st_as_sf(st_sfc(st_as_text(neighborhoods$the_geom)), crs = 4326)

# Extract the coordinates from the multipolygon object
coords <- st_coordinates(multipolygon)

# Calculate the average latitude and longitude
avg_lat <- mean(coords[, 2])  # Latitude values are in the second column
avg_long <- mean(coords[, 1]) # Longitude values are in the first column

# Print the average latitude and longitude
cat("Average Latitude:", avg_lat, "\n")
cat("Average Longitude:", avg_long, "\n")



end_stations <- clean_divvy %>%
  group_by(end_station_name) %>%
  summarise(
    avg_end_lat = mean(end_lat, na.rm = TRUE),
    avg_end_lng = mean(end_lng, na.rm = TRUE)
  )

```

```{r}

# # composite for uniqueness- no need anymore
divvy$startId <- paste0(divvy$start_station_id, "-", divvy$start_station_name)
divvy$endId <- paste0(divvy$end_station_id, "-", divvy$end_station_name)


# divvy$startId <- divvy$start_station_id
# divvy$endId <- divvy$end_station_id

station_starts <- divvy %>%
  count(startId) %>%
  rename(checkouts = n)

station_ends <- divvy %>%
  count(endId) %>%
  rename(checkins = n)

# fill missing w 0s
in_start <- setdiff(station_starts$startId, station_ends$endId)
end_missing <- data.frame(endId = in_start, checkins = rep(0, length(in_start)))
station_ends <- rbind(station_ends, end_missing)

in_end <- setdiff(station_ends$endId, station_starts$startId)
start_missing <- data.frame(startId = in_end, checkouts = rep(0, length(in_end)))
station_starts <- rbind(station_starts, start_missing)

station_counts <- merge(station_starts, station_ends, by.x = "startId", by.y = "endId", all = TRUE)
station_counts$total <- station_counts$checkouts + station_counts$checkins
colnames(station_counts) <- c("station_id", "checkouts", "checkins", "total")


top <- station_counts %>% arrange(desc(total)) %>%
  filter(total > 50000) # above 50,000 total checkins/outs
```

get quantiles
```{r}
quantile(station_counts$total)
hist_data <- hist(station_counts$total, 
     main = "Histogram of Check-Ins and Check-Outs, Divvy 2022-2023",  
     xlab = "Total",              
     ylab = "Frequency",         
     col = "lightblue",           
     border = "black",      
     breaks = 50,
     xaxt = "n") 
axis(1, at = hist_data$mids, labels = paste(
  "[", round(hist_data$breaks[-length(hist_data$breaks)]), 
  ",", round(hist_data$breaks[-1]), "]", sep=""), 
  cex.axis = 0.8)  # Adjust axis text size
```
Bottom 50% of stations have <= 316 rides. Label these as bottom and stations with >= 50,000 as top.
```{r}
stations_top <-  top$station_id
stations_bottom <- station_counts[station_counts$total <= 316,]$station_id

top_locs <- divvy %>%
  filter(startId %in% stations_top | endId %in% stations_top) %>% select(startId, endId, start_lat, start_lng, end_lat, end_lng)
bottom_locs <- divvy %>%
  filter(startId %in% stations_bottom | endId %in% stations_bottom) %>% select(startId, endId, start_lat, start_lng, end_lat, end_lng)

top_locs <- divvy %>%
  mutate(
    station_name = ifelse(startId %in% stations_top, startId, endId),
    station_lat = ifelse(startId %in% stations_top, start_lat, end_lat),
    station_long = ifelse(startId %in% stations_top, start_lng, end_lng)) %>% 
  select(station_name, station_lat, station_long) %>% distinct()

bottom_locs <- divvy %>%
  mutate(
    station_name = ifelse(startId %in% stations_bottom, startId, endId),
    station_lat = ifelse(startId %in% stations_bottom, start_lat, end_lat),
    station_long = ifelse(startId %in% stations_bottom, start_lng, end_lng)) %>% 
  select(station_name, station_lat, station_long) %>% distinct()

top_locs <- top_locs %>%
  mutate(station_type = "top")

bottom_locs <- bottom_locs %>%
  mutate(station_type = "bottom")

top_bottom <- bind_rows(top_locs, bottom_locs)
```

```{r}

library(ggplot2)
library(maps)
library(ggmap)


#world_map <- map_data("world")
chicago_map <- get_map(location = c(lon = -87.6298, lat = 41.8781), zoom = 12, source = "osm")
ggmap(chicago_map) + 
#ggplot(data = world_map) +
 # geom_polygon(aes(x = long, y = lat, group = group), fill = "lightgrey", color = "white") + 
  geom_point(data = top_bottom, aes(x = station_long, y = station_lat, color = factor(station_type)), size = 4) +  
  scale_color_manual(values = c("red", "blue"), labels = c("Top", "Bottom")) +
  theme_minimal() + 
  labs(title = "Map Plot with Top-Bottom Colored Points", color = "Top-Bottom") +
  coord_fixed(ratio = 1.1) #+
 # xlim(-88.0, -87.5) + ylim(41.6, 42.0)
write.csv(top_bottom, 'top_bottom.csv')
```



Get info for stations with > 50,000 total checkins/outs
```{r}
stations <- top$station_id
top_info <- divvy %>%
  filter(startId %in% stations | endId %in% stations)

library(lubridate)
# 
# top_info$started_at <- ymd_hms(top_info$started_at)
# top_info$ended_at <- ymd_hms(top_info$ended_at)


top_info$started_at <- ymd_hms(top_info$started_at, tz = "America/Chicago")
top_info$ended_at <- ymd_hms(top_info$ended_at, tz = "America/Chicago")
# 
# top_info$started_at <- !is.na(top_info$started_at)
# top_info$ended_at <- !is_na.(top_info$ended_at)
#head(top_info)
#colnames(top_info)

#daylight saviings
top_info <- top_info[top_info$started_at <= top_info$ended_at,]
```

% of rides covered by these stations
```{r}
# percent covered
total_trip_count <- nrow(divvy)
top_count <- nrow(top_info)
percent <- round((top_count / total_trip_count) * 100, 3)

percent 
length(unique(stations))
```
These top 121 stations cover 72% of the rides.

Model Info
```{r}
top_model <- top_info[, c('started_at', 'ended_at', 'startId', 'endId', 'member_casual',
                          'rideable_type')]

top_model$day <- weekdays(top_model$started_at)
top_model$year <- format(top_model$started_at, "%Y") 
top_model$hour <- format(top_model$started_at, "%H")
top_model$month <- format(top_model$started_at, "%B")

top_model$started_date <- as.Date(top_model$started_at)
top_model$ended_date <- as.Date(top_model$ended_at)

top_model$ride_length <- as.numeric(difftime(top_model$ended_at, top_model$started_at, units = "mins"))
```

Add weather

```{r}
load("CookCountyWeather0120_0724.RData")
weather <- weather_df
```

```{r}
weather <- read.csv("weather.csv")
weather$date <- as.Date(weather$date)
```

```{r}
merged_model <- merge(top_model, weather, by.x = "started_date", by.y = "date", all.x = TRUE)
```

```{r}
head(merged_model)
```


Prediction: The number of checkouts and checkins for these most popular stations.
```{r}
library(caret)
# create daily stats
checkouts <- merged_model %>%
  group_by(startId, started_date) %>%
  summarize(num_checkouts = n(),
         avg_ride_length = mean(ride_length, na.rm = TRUE),
         avg_ppt = mean(ppt, na.rm = TRUE),
         avg_temp = mean(tavg, na.rm = TRUE),
         perc_member = sum(member_casual == 'member')/n(),
         day = first(day),
         month = first(month)) %>% 
  ungroup()
# add lags for percent member, ride length, and num_checkouts
checkouts <- checkouts %>%
  group_by(startId) %>%
  arrange(startId, started_date) %>%
  mutate(prev_num_checkouts = lag(num_checkouts), 
    prev_avg_ride_length = lag(avg_ride_length),  
    prev_perc_member = lag(perc_member) ) %>%
  ungroup()

rownames(checkouts) <- paste0(checkouts$startId, "_", checkouts$started_date)
#head(checkouts)

checkins <- merged_model %>%
  group_by(endId, started_date) %>%
  summarize(num_checkins = n(),
         avg_ride_length = mean(ride_length, na.rm = TRUE),
         avg_ppt = mean(ppt, na.rm = TRUE),
         avg_temp = mean(tavg, na.rm = TRUE),
         perc_member = sum(member_casual == 'member')/n(),
         day = first(day),
         month = first(month)) %>% ungroup()
# add lags for percent member, ride length, and num_checkins
checkins <- checkins %>%
  group_by(endId) %>%
  arrange(endId, started_date) %>%
  mutate(prev_num_checkins = lag(num_checkins), 
    prev_avg_ride_length = lag(avg_ride_length),  
    prev_perc_member = lag(perc_member) ) %>%
  ungroup()

rownames(checkins) <- paste0(checkins$endId, "_", checkins$started_date)
```

dummy vars
```{r}
# cols to use for prediction: avg_ppt, avg_temp, day, month, prev_num_checkouts/ins, prev_avg_ride_length, prev_perc_member
# outcome var: num_checkouts/ins
dummy_modelOut <- dummyVars(~ month + day, data = checkouts)
checkouts_encoded <- predict(dummy_modelOut, newdata = checkouts)
checkouts_encoded <- as.data.frame(checkouts_encoded)
checkouts_final <- cbind(checkouts %>% 
                           select(avg_ppt, avg_temp, prev_num_checkouts, prev_avg_ride_length, prev_perc_member, num_checkouts),
                         checkouts_encoded)

dummy_modelIn <- dummyVars(~ month + day, data = checkins)
checkins_encoded <- predict(dummy_modelIn, newdata = checkins)
checkins_encoded <- as.data.frame(checkins_encoded)
checkins_final <- cbind(checkins %>% 
                          select(avg_ppt, avg_temp, prev_num_checkins, prev_avg_ride_length, prev_perc_member, num_checkins),
                        checkins_encoded)
```


XGBoost Model: Checkouts
```{r}
train_index <- sample(1:nrow(checkouts_final), size = 0.8 * nrow(checkouts_final))
train_data <- checkouts_final[train_index, ]
test_data <- checkouts_final[-train_index, ]

library(xgboost)

x_train <- train_data %>% select(-num_checkouts)
y_train <- train_data$num_checkouts

x_test <- test_data %>% select(-num_checkouts)
y_test <- test_data$num_checkouts

dtrain <- xgb.DMatrix(data = as.matrix(x_train), label = y_train)
dtest <- xgb.DMatrix(data = as.matrix(x_test), label = y_test)
```

```{r}
params <- list(
  objective = "reg:squarederror",
  eval_metric = "rmse",
  eta = 0.1)

# tune nrounds
nrounds <- xgboost(
  data = dtrain, 
  params = params, 
  nrounds = 1000,             
  early_stopping_rounds = 50,     
  print_every_n = 10,       
  verbose = 1)

best_nrounds <- nrounds$best_iteration

checkouts_model <- xgb.train(
  params = params,
  data = dtrain,
  nrounds = 1000, #best_nrounds,
  print_every_n = 10,
  verbose = 1)


y_pred <- predict(checkouts_model, newdata = dtest)
comp <- cbind(y_test, y_pred)


#importance_matrix <- xgb.importance(feature_names = colnames(x_train), model = checkouts_model)
#xgb.plot.importance(importance_matrix)
```

Eval
```{r}

results_out <- cbind(x_test, comp)
results_out <- results_out %>% rename(actual = y_test, pred = y_pred)
rows <- rownames(x_test) %>% strsplit('_') 
results_out <- cbind(unlist(rows)[2*(1:length(rows))-1], unlist(rows)[2*(1:length(rows))], results_out) %>% rename(id = `unlist(rows)[2 * (1:length(rows)) - 1]`, date = `unlist(rows)[2 * (1:length(rows))]`) %>% mutate(date = as.Date(date))
rownames(results_out) =NULL


plot <- sample_n(results_out, 10000)

# Plot
ggplot(plot, aes(x = actual, y = pred)) +
  geom_point() + 
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  labs(title = "Checkouts-- Predictions vs. Actual Values", x = "Actual Values", y = "Predicted Values") + 
  theme_minimal()

#Conf Int
# Calculate residuals 
results_out$residuals <- results_out$actual - results_out$pred

# Calculate margin of error for 95% confidence interval
critical_value <- 1.96
margin_of_error <- critical_value * sd(results_out$residuals)
# Check if actual values are within the confidence interval
results_out$within_interval <- (results_out$actual >= (results_out$pred - margin_of_error)) & (results_out$actual <= (results_out$pred + margin_of_error)) # Print results_out data.frame(Predictions = predictions, Actual = actual, Within_Interval = within_interval


percent_within <-  nrow(results_out[results_out$within_interval == TRUE,])*100/nrow(results_out)

percent_within
```
96% within 95% conf interval


Pricing
```{r }
 pricing <- top_model %>% mutate(ride_cost = ifelse(member_casual == "casual", ifelse(rideable_type == "electric_bike", ride_length*0.44 +1, ride_length*0.18 +1), ifelse(rideable_type == "electric_bike",ride_length*0.18, pmax(ride_length-45, 0) * 0.18)))%>%
  mutate(unlock_cost = ifelse(member_casual == "casual", 1, 0)) %>% group_by(startId, started_date) %>%
  summarise(revenue = sum(ride_cost), unlock_rev = sum(unlock_cost))

summary(pricing)
# 
# rev_model <- glm(revenue ~ checkouts + member_perc + avg_len + ebike_perc + , data = pricing)
# summary(rev_model)
# plot(rev_model)
```



XGBoost Model: Checkins
```{r}
train_index1 <- sample(1:nrow(checkins_final), size = 0.8 * nrow(checkins_final))
train_data1 <- checkins_final[train_index1, ]
test_data1 <- checkins_final[-train_index1, ]

x_train1 <- train_data1 %>% select(-num_checkins)
y_train1 <- train_data1$num_checkins

x_test1 <- test_data1 %>% select(-num_checkins)
y_test1 <- test_data1$num_checkins

dtrain1 <- xgb.DMatrix(data = as.matrix(x_train1), label = y_train1)
dtest1 <- xgb.DMatrix(data = as.matrix(x_test1), label = y_test1)
```




```{r}
params <- list(
  objective = "reg:squarederror",
  eval_metric = "rmse",
  eta = 0.1)

# tune nrounds
nrounds <- xgboost(
  data = dtrain1, 
  params = params, 
  nrounds = 1000,             
  early_stopping_rounds = 50,     
  print_every_n = 10,       
  verbose = 1)

best_nrounds <- nrounds$best_iteration

checkins_model <- xgb.train(
  params = params,
  data = dtrain1,
  nrounds = 1000, #best_nrounds,
  print_every_n = 10,
  verbose = 1)


y_pred1 <- predict(checkins_model, newdata = dtest1)
comp1 <- cbind(y_test1, y_pred1)


#importance_matrix <- xgb.importance(feature_names = colnames(x_train), model = checkins_model)
#xgb.plot.importance(importance_matrix)
```

```{r}

results_in <- cbind(x_test1, comp1)
results_in <- results_in %>% rename(actual = y_test1, pred = y_pred1)
rows_in <- rownames(x_test1) %>% strsplit('_') 
results_in <- cbind(unlist(rows_in)[2*(1:length(rows_in))-1], unlist(rows_in)[2*(1:length(rows_in))], results_in) %>% rename(id = `unlist(rows_in)[2 * (1:length(rows_in)) - 1]`, date = `unlist(rows_in)[2 * (1:length(rows_in))]`) %>% mutate(date = as.Date(date))
rownames(results_in) =NULL


plot_in <- sample_n(results_in, 10000)

# Plot
ggplot(plot_in, aes(x = actual, y = pred)) +
  geom_point() + 
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") + 
  labs(title = "CheckIns-- Predictions vs. Actual Values", x = "Actual Values", y = "Predicted Values") + 
  theme_minimal()

#Conf Int
# Calculate residuals 
results_in$residuals <- results_in$actual - results_in$pred

# Calculate margin of error for 95% confidence interval
critical_value <- 1.96
margin_of_error <- critical_value * sd(results_in$residuals)
# Check if actual values are within the confidence interval
results_in$within_interval <- (results_in$actual >= (results_in$pred - margin_of_error)) & (results_in$actual <= (results_in$pred + margin_of_error)) # Print results data.frame(Predictions = predictions, Actual = actual, Within_Interval = within_interval


percent_within_in <-  nrow(results_in[results_in$within_interval == TRUE,])*100/nrow(results_in)

percent_within_in
```


```{r}
in1 = rep(times = 103, NA)
out <- setdiff(results_out$id, results_in$id)
in1 <- setdiff(results_in$id, results_out$id)
df = data.frame(cbind(in1, out))
df$in1 <- strsplit(df$in1, "-")

in1 <- results_in %>% select(date, id, pred, actual) %>% rename(in_actual = actual, in_pred = pred)
out1 <- results_out %>% select(date, id, pred, actual) %>% rename(out_actual = actual, out_pred = pred)
pricing <- pricing %>% rename(id = startId, date = started_date) %>% select(-unlock_rev)
data <- left_join(inner_join(pricing, out1 , by = c("id", "date")), in1, by = c("id", "date")) %>% mutate_all(~replace(., is.na(.), 0))


# Example data frame
# data <- data.frame(
#   Number_of_Riders = c(50, 60, 70, 80, 90), #checkouts
#   Number_of_Drivers = c(30, 40, 50, 60, 70), #checkins
#   Historical_Cost_of_Ride = c(10, 15, 20, 25, 30)
# )
# Percentiles
high_demand_percentile <- 75
low_demand_percentile <- 25

# Calculate percentiles
high_demand_threshold <- quantile(data$out_actual, probs = high_demand_percentile / 100)
low_demand_threshold <- quantile(data$out_actual, probs = low_demand_percentile / 100)

# Calculate demand multiplier
data <- data %>%
  mutate(
    demand_multiplier = ifelse(
      out_actual > high_demand_threshold,
      out_actual / high_demand_threshold,
      out_actual / low_demand_threshold
    )
  )

# Percentiles
high_supply_percentile <- 75
low_supply_percentile <- 25

# Calculate percentiles
high_supply_threshold <- quantile(data$in_actual, probs = high_supply_percentile / 100)
low_supply_threshold <- quantile(data$in_actual, probs = low_supply_percentile / 100)

# Calculate supply multiplier
data <- data %>%
  mutate(
    supply_multiplier = ifelse(
      in_actual > low_supply_threshold,
      high_supply_threshold / in_actual,
      low_supply_threshold / in_actual
    )
  )

# Price adjustment factors
demand_threshold_high <- 1.2  # Higher demand threshold
demand_threshold_low <- 0.8   # Lower demand threshold
supply_threshold_high <- 0.8  # Higher supply threshold
supply_threshold_low <- 1.2   # Lower supply threshold

# Calculate adjusted ride cost
data <- data %>%
  mutate(
    adjusted_ride_cost = revenue * (
      pmax(demand_multiplier, demand_threshold_low) * 
      pmax(supply_multiplier, supply_threshold_high)
    )
  )

# View the result
print(data)

data <- data %>% mutate(profit_percentage = ((adjusted_ride_cost - revenue) / revenue) * 100)

# ID profitable rides
profitable_rides <- data %>% filter(profit_percentage > 0)

# ID rides incurring losses
loss_rides <- data %>% filter(profit_percentage <= 0)

# Calculate the count of profitable and loss rides
profitable_count <- nrow(profitable_rides)
loss_count <- nrow(loss_rides)

# Creating a donut chart
labels <- c('Profitable Rides', 'Loss Rides')
values <- c(profitable_count, loss_count)

library(plotly)
fig <- plot_ly(
  data = data.frame(labels, values),
  labels = ~labels,
  values = ~values,
  type = 'pie',
  hole = 0.4
)

fig <- fig %>%
  layout(
    title = 'Profitability of Rides (Dynamic Pricing vs Actual Pricing)',
    showlegend = TRUE
  )

fig
```
Shows that the demand pricing would be profitable.
Our model predicts 95-96%  of checkout/checkins daily numbers by stations within a 95% confidence interval. You can use our model to predict the checkins/checkouts for the following day. Based on those numbers you use the multipliers and threshold calculations to calculate whether you should surge pricing on that day for bikes coming out of certain racks.

Should calculate how our predictions over/under could affect e.g. on how many occasions did we predict that the demand would meet a change threshold and it didn't?

Can also cycle through different values for demand/supply threshold and percentile to see which is best.

Maybe look at demand based pricing on a station by station basis? The percentile is based on historic numbers of that station rather than total data set. Because otherwise it's always going to be the same stations with surge pricing (unless this is what we want?)

Source: https://medium.com/@mkinoti19/data-driven-dynamic-pricing-using-python-and-machine-learning-b898e0e5fbe3

```{r eval = FALSE}
test <- divvy

# List of substrings to remove
substr_to_remove <- c("Public Rack -", "Public  Rack -", "Pubic Rack -", "City Rack -", "\\*", "- Charging", "- midblock", "- midblock south", "\\(Temp\\)", "amp;") 
# Function to remove substrings
remove_substrings <- function(x, substrings) {
  for (substr in substrings) { x <- gsub(substr, "", x) } 
  return(x) } # Apply the function to the 'name' column 

divvy <- divvy %>% mutate(start_station_name = remove_substrings(start_station_name, substr_to_remove), end_station_name = remove_substrings(end_station_name, substr_to_remove) )


start_stat <- divvy[,c(4,5)] %>% distinct() %>% arrange(desc(start_station_name)) %>% 
  group_by(start_station_name) %>% slice_head(n = 1) %>% ungroup()


end_stat <- divvy[,c(6,7)] %>% distinct() %>% arrange(desc(end_station_name)) %>% 
  group_by(end_station_name) %>% slice_head(n = 1) %>% ungroup()


# test <- test %>% group_by(start_station_name) %>% mutate(unique_id = start_station_id[1]) %>% ungroup() %>% select(-start_station_id) %>% rename(start_station_id = unique_id)
# 
# 
# test <- test %>% group_by(end_station_name) %>% mutate(unique_id = end_station_id[1]) %>% ungroup() %>% select(-end_station_id) %>% rename(end_station_id = unique_id)

# 
# unique(test[test$start_station_name %in% setdiff(unique(test$start_station_name),unique(test$end_station_name)),]$start_station_name)
# 
# 
# unique(test[test$end_station_name %in%setdiff(unique(test$end_station_name),unique(test$start_station_name)),]$end_station_name)

duplicates <- start_stat %>% group_by(start_station_name) %>% filter(n_distinct(start_station_id) > 1) %>% ungroup() %>% select(start_station_id, start_station_name) %>% distinct() %>% arrange(desc(start_station_name))

duplicates <- end_stat %>% group_by(end_station_name) %>% filter(n_distinct(end_station_id) > 1) %>% ungroup() %>% select(end_station_id, end_station_name) %>% distinct() %>% arrange(desc(end_station_name))


start_stat <- test[,c(4,5)] %>% distinct() %>% arrange(desc(start_station_name)) %>% 
  group_by(start_station_name) %>% slice_head(n = 1) %>% ungroup()


end_stat <- test[,c(6,7)] %>% distinct() %>% arrange(desc(end_station_name)) %>% 
  group_by(end_station_name) %>% slice_head(n = 1) %>% ungroup()

stats <- full_join(start_stat, end_stat, by = c("start_station_name" = "end_station_name"))
summary(stats[stats$start_station_id != stats$end_station_id,])

duplicates <- stats %>%  group_by(start_station_id) %>% filter(n() > 1) %>% ungroup()%>% arrange(start_station_id)
  
  stats[is.na(stats$start_station_id),]
  stats[is.na(stats$end_station_id),]
  test[test$end_station_name =='Nordica Ave & Addison St',]
```

